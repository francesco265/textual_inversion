{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "RuGaaeNtq4lP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "69lqDjs4A_p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install diffusers accelerate safetensors"
      ],
      "metadata": {
        "id": "MQC9mlto4JL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable diffusion pipeline"
      ],
      "metadata": {
        "id": "VQsUoVx_bQkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "scheduler = DDPMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "pipe.safety_checker = None"
      ],
      "metadata": {
        "id": "Qf9kQH-xk8kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textual inversion\n",
        "Classi necessarie per il training e training della Textual Inversion classica"
      ],
      "metadata": {
        "id": "RuGaaeNtq4lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from types import MethodType\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "\n",
        "prompts = [\n",
        "    \"a painting in the style of {}\",\n",
        "    \"a rendering in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"the painting in the style of {}\",\n",
        "    \"a clean painting in the style of {}\",\n",
        "    \"a dirty painting in the style of {}\",\n",
        "    \"a dark painting in the style of {}\",\n",
        "    \"a picture in the style of {}\",\n",
        "    \"a cool painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a bright painting in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"a good painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a rendition in the style of {}\",\n",
        "    \"a nice painting in the style of {}\",\n",
        "    \"a small painting in the style of {}\",\n",
        "    \"a weird painting in the style of {}\",\n",
        "    \"a large painting in the style of {}\"\n",
        "]\n",
        "\n",
        "class ArtistDataset(TorchDataset):\n",
        "  def __init__(self, data_root, tokenizer, placeholder_token, clip_output=False,\n",
        "               size=512, repeats=100, flip_p=0.5, device=\"cpu\"):\n",
        "    self.data_root = data_root\n",
        "    self.tokenizer = tokenizer\n",
        "    self.size = size\n",
        "    self.placeholder_token = placeholder_token\n",
        "    self.device = device\n",
        "    self.flip_p = flip_p\n",
        "    self.clip_output = clip_output\n",
        "\n",
        "    self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
        "\n",
        "    self.num_images = len(self.image_paths)\n",
        "    self._length = self.num_images * repeats\n",
        "\n",
        "    self.templates = prompts\n",
        "    self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
        "    self.resize = transforms.Resize(self.size)\n",
        "    self.crop = transforms.CenterCrop(self.size)\n",
        "\n",
        "    # CLIP\n",
        "    self.clip_resize = transforms.Resize(224)\n",
        "    self.norm_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])\n",
        "    self.norm_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])\n",
        "  def get_artworks_for_eval(self):\n",
        "    return [Image.open(x) for x in self.image_paths]\n",
        "  def __len__(self):\n",
        "    return self._length\n",
        "  def __getitem__(self, i):\n",
        "    example = {}\n",
        "    image = Image.open(self.image_paths[i % self.num_images])\n",
        "\n",
        "    if not image.mode == \"RGB\":\n",
        "      image = image.convert(\"RGB\")\n",
        "\n",
        "    if self.tokenizer:\n",
        "      text = random.choice(self.templates).format(self.placeholder_token)\n",
        "      tokenized = self.tokenizer(text,\n",
        "                                 padding=\"max_length\",\n",
        "                                 truncation=True,\n",
        "                                 max_length=self.tokenizer.model_max_length,\n",
        "                                 return_tensors=\"pt\")\n",
        "      example[\"input_ids\"] = tokenized.input_ids[0].to(self.device)\n",
        "\n",
        "    image = image.resize((self.size, self.size))\n",
        "    # image = self.resize(image)\n",
        "    # image = self.crop(image)\n",
        "    image = self.flip_transform(image)\n",
        "    image = transforms.functional.to_tensor(image)\n",
        "    #image = np.array(image).astype(np.uint8)\n",
        "    #image = (image / 127.5 - 1.0).astype(np.float16)\n",
        "    if self.clip_output:\n",
        "      clip_input = {}\n",
        "      clip_image = self.clip_resize(image)\n",
        "      clip_image = (clip_image - self.norm_mean[:,None,None]) / self.norm_std[:,None,None]\n",
        "      clip_input[\"pixel_values\"] = clip_image.to(self.device, dtype=torch.float16)\n",
        "\n",
        "    image = (2 * image - 1).to(self.device, dtype=torch.float16)\n",
        "\n",
        "    #example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1).to(self.device)\n",
        "    example[\"pixel_values\"] = image\n",
        "\n",
        "    if self.clip_output:\n",
        "      return example, clip_input\n",
        "    return example\n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self, data_root, tokenizer=None, device=\"cpu\", clip_output=False):\n",
        "    self.data_root = data_root\n",
        "    self.artist_list = sorted(os.listdir(self.data_root))\n",
        "    self.artist_iterables = {\n",
        "        f\"{x}\" : ArtistDataset(os.path.join(self.data_root, x), tokenizer, f\"<{x}>\", clip_output=clip_output, device=device)\n",
        "        for x in self.artist_list\n",
        "        }\n",
        "  def get_artists(self):\n",
        "    return self.artist_list\n",
        "  def get_artists_num(self):\n",
        "    return len(self.artist_list)\n",
        "  def __getitem__(self, artist):\n",
        "    assert artist in self.artist_list, f\"{artist} is not present in the dataset\"\n",
        "    return self.artist_iterables[artist]\n",
        "\n",
        "\n",
        "def forward(self, input_ids = None, position_ids = None, inputs_embeds = None) -> torch.Tensor:\n",
        "  vocab_size = self.token_embedding.num_embeddings\n",
        "  seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
        "\n",
        "  if position_ids is None:\n",
        "    position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "  if inputs_embeds is None:\n",
        "    concept_pos = input_ids >= vocab_size\n",
        "    input_ids[concept_pos] -= vocab_size\n",
        "\n",
        "    inputs_embeds = self.token_embedding(input_ids)\n",
        "    if concept_pos.any():\n",
        "      inputs_embeds[concept_pos,:] = self.concept_embedding(input_ids[concept_pos])\n",
        "      input_ids[concept_pos] += vocab_size\n",
        "\n",
        "  position_embeddings = self.position_embedding(position_ids)\n",
        "  embeddings = inputs_embeds + position_embeddings\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "xqC7hr4HTXwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "5tc0aJXtwHGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from safetensors.torch import save_file\n",
        "from math import ceil\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import mse_loss\n",
        "from accelerate import Accelerator\n",
        "from tqdm import tqdm\n",
        "\n",
        "MAX_EPOCHS = 1000\n",
        "LR = 0.005\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# Custom forward function\n",
        "obj = pipe.text_encoder.text_model.embeddings\n",
        "obj.forward = MethodType(forward, obj)\n",
        "\n",
        "# Dataset\n",
        "dataset = Dataset('train/', pipe.tokenizer, device)\n",
        "\n",
        "# unet and vae are converted to fp16 while the text_encoder keeps fp32\n",
        "accelerator = Accelerator(mixed_precision='fp16')\n",
        "vae = pipe.vae.to(accelerator.device, dtype=torch.float16)\n",
        "unet = pipe.unet.to(accelerator.device, dtype=torch.float16)\n",
        "text_encoder = pipe.text_encoder\n",
        "\n",
        "# Add new tokens for each artist and their embeddings\n",
        "pipe.tokenizer.add_tokens(list(map(lambda x: f'<{x}>', dataset.get_artists())))\n",
        "text_encoder.text_model.embeddings.concept_embedding = nn.Embedding(dataset.get_artists_num(), 768, device=text_encoder.device)\n",
        "\n",
        "# Freeze parameters\n",
        "for m in [vae, unet, text_encoder]:\n",
        "  for param in m.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unlock only concept embeddings\n",
        "text_encoder.text_model.embeddings.concept_embedding.weight.requires_grad = True\n",
        "text_encoder.train()\n",
        "opt = AdamW(text_encoder.text_model.embeddings.concept_embedding.parameters(), LR)\n",
        "\n",
        "text_encoder, opt = accelerator.prepare(text_encoder, opt)\n",
        "\n",
        "random.seed(4316)\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(4316)\n",
        "# Train\n",
        "for artist in dataset.get_artists():\n",
        "  data_loader = DataLoader(dataset[artist], BATCH_SIZE, True, generator=generator)\n",
        "  data_loader = accelerator.prepare(data_loader)\n",
        "  num_batches = len(data_loader)\n",
        "  num_epochs = ceil(MAX_EPOCHS / num_batches)\n",
        "\n",
        "  pbar = tqdm(range(num_epochs), f\"{artist}\")\n",
        "  for epoch in pbar:\n",
        "    for i, batch in enumerate(data_loader):\n",
        "      # Get image latents\n",
        "      with torch.no_grad():\n",
        "        latents = vae.encode(batch['pixel_values']).latent_dist.sample(generator=generator) * pipe.vae.config.scaling_factor\n",
        "      bsz = latents.shape[0]\n",
        "\n",
        "      # Sample noise\n",
        "      noise = torch.randn_like(latents)\n",
        "\n",
        "      # Sample timesteps\n",
        "      timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "      timesteps = timesteps.long()\n",
        "\n",
        "      # Add noise\n",
        "      noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "      # Text encoding\n",
        "      # text_embeddings = text_encoder(batch['input_ids'], batch['attention_mask'])[0].to(latents.dtype)\n",
        "      text_embeddings = text_encoder(batch['input_ids'])[0].to(latents.dtype)\n",
        "\n",
        "      # Model prediction\n",
        "      pred = unet(noisy_latents, timesteps, text_embeddings).sample\n",
        "\n",
        "      loss = mse_loss(pred.float(), noise.float())\n",
        "      accelerator.backward(loss)\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "      pbar.set_postfix({'loss': loss.item()})\n",
        "      pbar.set_postfix({'batch': f\"{i}/{num_batches}\"})\n",
        "\n",
        "  # Save style embeddings (float32)\n",
        "  print(\"Saving embeddings...\")\n",
        "  save_file(\n",
        "      {'concept_embedding': text_encoder.text_model.embeddings.concept_embedding.weight},\n",
        "      'drive/MyDrive/styles.safetensors'\n",
        "  )\n",
        "\n",
        "text_encoder.eval()\n",
        "# After training convert text_encoder's weights to float16\n",
        "text_encoder.to(dtype=torch.float16)"
      ],
      "metadata": {
        "id": "mq_DzUhYwGiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textual Inversion - Custom\n",
        "Training delle varianti di Textual Inversion:\n",
        "- Textual Inversion con Attention layers\n",
        "- Textual Inversion con Mixture of Experts"
      ],
      "metadata": {
        "id": "4E7yWQv8gKcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from types import MethodType\n",
        "\n",
        "class AttentionEmbedder(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.cls = nn.Parameter(torch.randn((1, embed_dim)))\n",
        "    self.attn_1 = nn.MultiheadAttention(embed_dim, num_heads, dropout=0.1, bias=False)\n",
        "    self.attn_2 = nn.MultiheadAttention(embed_dim, num_heads, dropout=0.1, bias=False)\n",
        "    self.mlp = nn.Linear(embed_dim, embed_dim)\n",
        "    self.act = nn.GELU()\n",
        "  def forward(self, x, need_weights=False):\n",
        "    # x -> (n, d)\n",
        "    x = torch.cat([self.cls, x], dim=0)\n",
        "    x_1, coefs_1 = self.attn_1(x, x, x, need_weights=need_weights)\n",
        "    x_2, coefs_2 = self.attn_2(x_1, x, x, need_weights=need_weights)\n",
        "\n",
        "    return self.mlp(self.act(x_2[0])), coefs_1, coefs_2\n",
        "\n",
        "class Expert(nn.Module):\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_features, in_features * 2, bias=False)\n",
        "    self.l2 = nn.Linear(in_features * 2, in_features, bias=False)\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    self.act = nn.GELU()\n",
        "  def forward(self, x):\n",
        "    return self.l2(self.dropout(self.act(self.l1(x))))\n",
        "\n",
        "class MoE(nn.Module):\n",
        "  def __init__(self, in_features, n_experts=4, top_k=2):\n",
        "    super().__init__()\n",
        "    self.experts = nn.ModuleList([Expert(in_features) for _ in range(n_experts)])\n",
        "    self.n_experts = n_experts\n",
        "    self.top_k = top_k\n",
        "    # Router\n",
        "    self.noise_std = 1 / n_experts\n",
        "    self.router = nn.Linear(in_features, n_experts, bias=False)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.dist = torch.distributions.Normal(0, self.noise_std)\n",
        "    # Compress\n",
        "    self.dropout = nn.Dropout(p=0.15)\n",
        "    self.compress = nn.Linear(in_features * 4, in_features)\n",
        "  def forward(self, x, coef=False):\n",
        "    bsz, dim = x.shape\n",
        "\n",
        "    G_x = self.router(x)\n",
        "\n",
        "    # Importance loss\n",
        "    imp_loss = self.softmax(G_x).sum(0)\n",
        "    imp_loss = (imp_loss.std() / imp_loss.mean()).pow(2)\n",
        "\n",
        "    G_x_noised = G_x + (torch.randn_like(G_x) * self.noise_std)\n",
        "\n",
        "    # Load loss\n",
        "    threshold_k = G_x_noised.kthvalue(self.n_experts - self.top_k + 1, dim=1, keepdim=True)\n",
        "    load_loss = (1 - self.dist.cdf((threshold_k.values - G_x))).sum(0)\n",
        "    load_loss = (load_loss.std() / load_loss.mean()).pow(2)\n",
        "\n",
        "    G_x_noised = self.softmax(G_x_noised)\n",
        "    topk = torch.topk(G_x_noised, self.top_k, dim=1)\n",
        "\n",
        "    tmp = torch.zeros_like(G_x_noised)\n",
        "    tmp[torch.arange(bsz).view(-1,1), topk.indices] = topk.values\n",
        "    G_x_noised = tmp\n",
        "\n",
        "    # y -> (batch, n_experts, dim)\n",
        "    y = torch.zeros((bsz,self.n_experts,dim), dtype=x.dtype, device=x.device)\n",
        "    for i, expert in enumerate(self.experts):\n",
        "      non_zero = (G_x_noised[:,i] != 0)\n",
        "      if non_zero.any():\n",
        "        y[non_zero,i,:] = expert(x[non_zero])\n",
        "\n",
        "    result = torch.einsum('ijk,ij->ik', y, G_x_noised)\n",
        "    result = self.compress(self.dropout(result.view(-1)))\n",
        "\n",
        "    if coef:\n",
        "      return result, G_x_noised\n",
        "    return result, 0.5 * (imp_loss + load_loss)\n",
        "\n",
        "def forward(self, input_ids = None, position_ids = None, inputs_embeds = None) -> torch.Tensor:\n",
        "  vocab_size = self.token_embedding.num_embeddings\n",
        "  seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
        "\n",
        "  if position_ids is None:\n",
        "    position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "  if inputs_embeds is None:\n",
        "    concept_pos = input_ids >= vocab_size\n",
        "    input_ids[concept_pos] -= vocab_size\n",
        "\n",
        "    inputs_embeds = self.token_embedding(input_ids)\n",
        "    if concept_pos.any():\n",
        "      inputs_embeds[concept_pos,:] = self.vision_input\n",
        "      input_ids[concept_pos] += vocab_size\n",
        "\n",
        "  position_embeddings = self.position_embedding(position_ids)\n",
        "  embeddings = inputs_embeds + position_embeddings\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "H_VSa1H-gUgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "fqvQhe5yYvRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from safetensors.torch import save_file\n",
        "from math import ceil\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import mse_loss\n",
        "from accelerate import Accelerator\n",
        "from tqdm import tqdm\n",
        "\n",
        "MAX_EPOCHS = 1000\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 4\n",
        "MODEL = 'moe'     # moe or attention\n",
        "\n",
        "# Custom forward function\n",
        "obj = pipe.text_encoder.text_model.embeddings\n",
        "obj.forward = MethodType(forward, obj)\n",
        "\n",
        "# Dataset\n",
        "dataset = Dataset('train/', pipe.tokenizer, device, clip_output=True)\n",
        "\n",
        "# unet and vae are converted to fp16 while the text_encoder keeps fp32\n",
        "accelerator = Accelerator(mixed_precision='fp16')\n",
        "vae = pipe.vae.to(accelerator.device, dtype=torch.float16)\n",
        "unet = pipe.unet.to(accelerator.device, dtype=torch.float16)\n",
        "vision_model = clip.vision_model.to(accelerator.device)#, dtype=torch.float16)\n",
        "visual_projection = clip.visual_projection.to(accelerator.device)#, dtype=torch.float16)\n",
        "text_encoder = pipe.text_encoder\n",
        "\n",
        "# Add new tokens for each artist and their embeddings\n",
        "pipe.tokenizer.add_tokens(list(map(lambda x: f'<{x}>', dataset.get_artists())))\n",
        "concept_embeddings = torch.zeros((dataset.get_artists_num(), 768), device=accelerator.device)\n",
        "\n",
        "# Freeze parameters\n",
        "for m in [vision_model, visual_projection, vae, unet, text_encoder]:\n",
        "  for param in m.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "text_encoder.train()\n",
        "text_encoder = accelerator.prepare(text_encoder)\n",
        "# Train\n",
        "for j, artist in enumerate(dataset.get_artists()):\n",
        "  # Reproducibility\n",
        "  torch.manual_seed(4316)\n",
        "  random.seed(4316)\n",
        "  if MODEL == 'moe':\n",
        "    embedder = MoE(768)\n",
        "  else:\n",
        "    embedder = AttentionEmbedder(768, 8)\n",
        "  embedder.to(accelerator.device)\n",
        "  embedder.train()\n",
        "  opt = AdamW(embedder.parameters(), LR)\n",
        "\n",
        "  data_loader = DataLoader(dataset[artist], BATCH_SIZE, True)\n",
        "  num_batches = len(data_loader)\n",
        "  num_epochs = ceil(MAX_EPOCHS / num_batches)\n",
        "\n",
        "  clip_emb = clip_processor(images=dataset[artist].get_artworks_for_eval(), return_tensors='pt')\n",
        "  clip_emb = visual_projection(vision_model(clip_emb['pixel_values'].to(device)).pooler_output)\n",
        "\n",
        "  #data_loader, embedder, opt = accelerator.prepare(data_loader, embedder, opt)\n",
        "  data_loader, opt = accelerator.prepare(data_loader, opt)\n",
        "  pbar = tqdm(range(num_epochs), f\"{artist}\")\n",
        "  for epoch in pbar:\n",
        "    for i, (batch, clip_input) in enumerate(data_loader):\n",
        "      # Get image latents\n",
        "      with torch.no_grad():\n",
        "        latents = vae.encode(batch['pixel_values']).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
        "      bsz = latents.shape[0]\n",
        "\n",
        "      # Sample noise\n",
        "      noise = torch.randn_like(latents)\n",
        "\n",
        "      # Sample timesteps\n",
        "      timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "      timesteps = timesteps.long()\n",
        "\n",
        "      # Add noise\n",
        "      noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "      # Text encoding\n",
        "      #clip_emb = vision_model(clip_input['pixel_values']).pooler_output\n",
        "      #clip_emb = visual_projection(clip_emb)\n",
        "      embedder_out = embedder(clip_emb)\n",
        "      token_emb = embedder_out[0]\n",
        "      text_encoder.text_model.embeddings.vision_input = token_emb.repeat(bsz, 1)\n",
        "      text_embeddings = text_encoder(batch['input_ids'])[0].to(latents.dtype)\n",
        "\n",
        "      # Model prediction\n",
        "      pred = unet(noisy_latents, timesteps, text_embeddings).sample\n",
        "\n",
        "      loss = mse_loss(pred.float(), noise.float())\n",
        "      if MODEL == 'moe':\n",
        "        loss += 0.01 * embedder_out[1]\n",
        "      accelerator.backward(loss)\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "      pbar.set_postfix({'loss': loss.item()})\n",
        "      pbar.set_postfix({'batch': f\"{i}/{num_batches}\"})\n",
        "\n",
        "  # Generate style embeddings\n",
        "  concept_embeddings[j,:] = embedder(clip_emb)[0]\n",
        "\n",
        "  # Save style embeddings (float32)\n",
        "  print(\"Saving embeddings...\")\n",
        "  save_file(\n",
        "      {'concept_embedding': concept_embeddings},\n",
        "      'drive/MyDrive/styles_moe.safetensors'\n",
        "  )\n",
        "\n",
        "text_encoder.eval()\n",
        "embedder.eval()\n",
        "# After training convert text_encoder's weights to float16\n",
        "text_encoder.to(dtype=torch.float16)"
      ],
      "metadata": {
        "id": "y1cv9pbDYq-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP evaluation\n",
        "Valutazione quantitativa dei risultati"
      ],
      "metadata": {
        "id": "EQG7WiIq7WZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caricamento degli embeddings salvati in *styles.safetensors*"
      ],
      "metadata": {
        "id": "E_OYhi0Cdpmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors import safe_open\n",
        "\n",
        "# This should be the same one used during training\n",
        "dataset = Dataset('train/', None, device)\n",
        "\n",
        "new_tokens = pipe.tokenizer.add_tokens(list(map(lambda x: f'<{x}>', dataset.get_artists())))\n",
        "pipe.text_encoder.resize_token_embeddings(pipe.text_encoder.config.vocab_size + new_tokens)\n",
        "\n",
        "with safe_open(\"styles.safetensors\", framework=\"pt\", device=device) as f:\n",
        "  style_emb = f.get_tensor('concept_embedding')\n",
        "\n",
        "pipe.text_encoder.text_model.embeddings.token_embedding.requires_grad_(False)\n",
        "pipe.text_encoder.text_model.embeddings.token_embedding.weight[-new_tokens:,:] = style_emb\n",
        "pipe.to(device, dtype=torch.float16)"
      ],
      "metadata": {
        "id": "HQnw33lGlzEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation loop"
      ],
      "metadata": {
        "id": "4cxesz7yl3Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "eval_prompts = [\n",
        "    'A portrait of a large family in the style of {}',\n",
        "    'A close up of a young man in the style of {}',\n",
        "    'A naturalistic landscape in the style of {}',\n",
        "    'A landscape of a city in the style of {}'\n",
        "]\n",
        "\n",
        "dataset = Dataset('train/', None, device)\n",
        "prompts = clip_processor(text=list(map(lambda x: x.replace('in the style of {}', '').strip(), eval_prompts)),\n",
        "                         return_tensors='pt', padding=True)\n",
        "# Initialize result tables\n",
        "acc = {'Artist': []}\n",
        "acc.update({f'prompt_{i + 1}': [] for i in range(len(eval_prompts))})\n",
        "edi = {'Artist': []}\n",
        "edi.update({f'prompt_{i + 1}': [] for i in range(len(eval_prompts))})\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompts_emb = clip.get_text_features(**prompts)\n",
        "  prompts_emb = prompts_emb / prompts_emb.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "for artist_name in tqdm(dataset.get_artists()):\n",
        "  acc['Artist'].append(artist_name)\n",
        "  edi['Artist'].append(artist_name)\n",
        "\n",
        "  artist = dataset[artist_name]\n",
        "  artworks = artist.get_artworks_for_eval()\n",
        "  artworks = clip_processor(images=artworks, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    artworks_emb = clip.get_image_features(**artworks)\n",
        "    artworks_emb = artworks_emb / artworks_emb.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "  generator = torch.Generator(device)\n",
        "  generator.manual_seed(4316)\n",
        "  for i, prompt in enumerate(eval_prompts):\n",
        "    sd_out = pipe(prompt.format(artist.placeholder_token),\n",
        "    #sd_out = pipe(prompt.format(' '.join([x.capitalize() for x in artist_name.split('-')])),\n",
        "                  num_images_per_prompt=8,\n",
        "                  generator=generator).images\n",
        "    sd_out = clip_processor(images=sd_out, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "      sd_out_emb = clip.get_image_features(**sd_out)\n",
        "      sd_out_emb = sd_out_emb / sd_out_emb.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "    # Similarity between real and generated artworks (accuracy)\n",
        "    acc_score = torch.matmul(artworks_emb, sd_out_emb.t())\n",
        "    acc_score = acc_score.mean(dim=1).max().item() # o usare la media?\n",
        "    acc[f'prompt_{i + 1}'].append(acc_score)\n",
        "\n",
        "    # Similarity between textual prompt and generated artworks (editability)\n",
        "    edi_score = torch.matmul(prompts_emb[i], sd_out_emb.t())\n",
        "    edi_score = edi_score.mean().item()\n",
        "    edi[f'prompt_{i + 1}'].append(edi_score)\n",
        "\n",
        "acc_table = pd.DataFrame(acc)\n",
        "edi_table = pd.DataFrame(edi)\n",
        "acc_table.to_csv('accuracy.tsv', sep='\\t')\n",
        "edi_table.to_csv('editability.tsv', sep='\\t')"
      ],
      "metadata": {
        "id": "V4fZrZgC7ZVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Immagini generate per artista\n",
        "Genera un immagine per ogni artista/prompt per la valutazione visiva.\n",
        "Gli embedding devono essere già caricati nel modello."
      ],
      "metadata": {
        "id": "1SxL3EFXdN4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import choice\n",
        "\n",
        "def add_headers(\n",
        "    fig,\n",
        "    *,\n",
        "    row_headers=None,\n",
        "    col_headers=None,\n",
        "    row_pad=5,\n",
        "    col_pad=5,\n",
        "    rotate_row_headers=True,\n",
        "    **text_kwargs\n",
        "):\n",
        "  axes = fig.get_axes()\n",
        "\n",
        "  for ax in axes:\n",
        "    sbs = ax.get_subplotspec()\n",
        "\n",
        "    # Putting headers on cols\n",
        "    if (col_headers is not None) and sbs.is_first_row():\n",
        "      ax.annotate(\n",
        "        col_headers[sbs.colspan.start],\n",
        "        xy=(0.5, 1),\n",
        "        xytext=(0, col_pad),\n",
        "        xycoords=\"axes fraction\",\n",
        "        textcoords=\"offset points\",\n",
        "        ha=\"center\",\n",
        "        va=\"baseline\",\n",
        "        **text_kwargs,\n",
        "      )\n",
        "\n",
        "    # Putting headers on rows\n",
        "    if (row_headers is not None) and sbs.is_first_col():\n",
        "      ax.annotate(\n",
        "        row_headers[sbs.rowspan.start],\n",
        "        xy=(0, 0.5),\n",
        "        xytext=(-ax.yaxis.labelpad - row_pad, 0),\n",
        "        #xycoords=ax.yaxis.label,\n",
        "        xycoords=\"axes fraction\",\n",
        "        textcoords=\"offset points\",\n",
        "        ha=\"right\",\n",
        "        va=\"center\",\n",
        "        rotation=rotate_row_headers * 90,\n",
        "        **text_kwargs,\n",
        "      )\n",
        "\n",
        "prompts = [\n",
        "    'A portrait of a large family in the style of {}',\n",
        "    'A close up of a young man in the style of {}',\n",
        "    'A naturalistic landscape in the style of {}',\n",
        "    'A landscape of a city in the style of {}'\n",
        "]\n",
        "\n",
        "dataset = Dataset('train/', None, device)\n",
        "images = []\n",
        "\n",
        "for artist_name in dataset.get_artists():\n",
        "  artist = dataset[artist_name]\n",
        "  images.append(choice(dataset[artist_name].get_artworks_for_eval()))\n",
        "  for prompt in prompts:\n",
        "    sd_out = pipe(prompt.format(artist.placeholder_token)).images\n",
        "    #sd_out = pipe(prompt.format(' '.join([x.capitalize() for x in artist_name.split('-')])))\n",
        "    images.append(sd_out[0])\n",
        "\n",
        "images_np = [np.array(img.resize((128,128))) for img in images]\n",
        "\n",
        "n_cols = len(prompts) + 1\n",
        "n_rows = dataset.get_artists_num()\n",
        "spacing = 5\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(14, 25))\n",
        "\n",
        "for idx, ax in enumerate(axs.reshape(-1)):\n",
        "  ax.imshow(images_np[idx])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=spacing/100, hspace=spacing/100)\n",
        "\n",
        "prompts = ['Original'] + [p.replace('in the style of {}', '').strip() for p in prompts]\n",
        "row_labels = [' '.join([x.capitalize() for x in y.split('-')]) for y in dataset.get_artists()]\n",
        "add_headers(fig, col_headers=prompts, row_headers=row_labels, fontsize=12)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "aJqpEAMBdWVn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}